{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c4abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib import request\n",
    "from urllib.request import urlopen\n",
    "import os.path\n",
    "import nltk \n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "list1=[]\n",
    "def doc(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "dm = doc(url)\n",
    "soup = BeautifulSoup(dm, 'html.parser')\n",
    "for link in soup.find_all('a',attrs={'href': re.compile(\"^https://\")}):\n",
    "    \n",
    "    l=link.get('href') \n",
    "    list.append(l)\n",
    "print(sum)\n",
    "count=0\n",
    "loop=0\n",
    "for i in range(len(list)):\n",
    "    url1=list[i]\n",
    "    response = requests.get(url1)\n",
    "    text=response.text\n",
    "    soup1 = BeautifulSoup(text, 'html.parser')\n",
    "    for link1 in soup1.find_all('a',attrs={'href': re.compile(\"^https://\")}):\n",
    "        l1=link1.get('href') \n",
    "        list1.append(l)\n",
    "        count+=1\n",
    "        loop+=1\n",
    "        print(l1)\n",
    "    if loop>10000:     \n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "print(count)\n",
    "print(loop)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda768f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list1)):\n",
    "    url2 = list1[i]\n",
    "    tf=[url2]\n",
    "    lm=WordNetLemmatizer()\n",
    "    try:\n",
    "        for lines in urlopen(url2):        \n",
    "            s=re.sub('[^a-zA-Z]',' ',str(lines))\n",
    "            s=s.lower()\n",
    "            s=s.split()\n",
    "            s=[lm.lemmatize(word) for word in s if word not in stopwords.words('english')]\n",
    "            s=' '.join(s)\n",
    "            tf.append(s)    \n",
    "    except:\n",
    "        pass\n",
    "    print(tf)\n",
    "    save_pth = r'C:\\Users\\Ramesh kumar\\Desktop\\memphis notes\\IR\\Assign-6\\files'\n",
    "    filenm = \"file\"+str(i)+\".txt\"\n",
    "    n = os.path.join(save_pth, filenm)\n",
    "    f = open(n, \"w\")\n",
    "    f.write(str(tf))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
